{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LIVE: Hyper-param tuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinay10949/AnalyticsAndML/blob/master/Python/LIVE_Hyper_param_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk9_bfVXmXzj",
        "colab_type": "text"
      },
      "source": [
        "## Grid-Search\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUZjUtEbbd_s",
        "colab_type": "code",
        "outputId": "2b177182-b112-4a65-d0f6-de2983c23651",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Source: http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n",
        "# GridSearch\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "\n",
        "# Loading the Digits dataset\n",
        "digits = datasets.load_digits()\n",
        "\n",
        "# To apply an classifier on this data, we need to flatten the image, to\n",
        "# turn the data in a (samples, feature) matrix:\n",
        "n_samples = len(digits.images)\n",
        "X = digits.images.reshape((n_samples, -1))\n",
        "y = digits.target\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "\n",
        "# Split the dataset in two equal parts\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.5, random_state=0)\n",
        "\n",
        "# Set the parameters by cross-validation\n",
        "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
        "                     'C': [1, 10, 100, 1000]},\n",
        "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
        "\n",
        "scores = ['precision', 'recall']\n",
        "\n",
        "for score in scores:\n",
        "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
        "    print()\n",
        "\n",
        "    clf = GridSearchCV(\n",
        "        SVC(), tuned_parameters, scoring='%s_macro' % score\n",
        "    )\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    print(\"Best parameters set found on train set:\")\n",
        "    print()\n",
        "    print(clf.best_params_)\n",
        "    print()\n",
        "    print(\"Grid scores on train set:\")\n",
        "    print()\n",
        "    means = clf.cv_results_['mean_test_score']\n",
        "    stds = clf.cv_results_['std_test_score']\n",
        "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
        "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
        "              % (mean, std * 2, params))\n",
        "    print()\n",
        "\n",
        "    print(\"Detailed classification report on test-set:\")\n",
        "    print()\n",
        "    #print(\"The model is trained on the full development set.\")\n",
        "    #print(\"The scores are computed on the full evaluation set.\")\n",
        "    print()\n",
        "    y_true, y_pred = y_test, clf.predict(X_test)\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print()\n",
        "\n",
        "# Note the problem is too easy: the hyperparameter plateau is too flat and the\n",
        "# output model is the same for precision and recall with ties in quality.\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Automatically created module for IPython interactive environment\n",
            "(1797, 64)\n",
            "(1797,)\n",
            "# Tuning hyper-parameters for precision\n",
            "\n",
            "Best parameters set found on train set:\n",
            "\n",
            "{'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "\n",
            "Grid scores on train set:\n",
            "\n",
            "0.986 (+/-0.016) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.959 (+/-0.028) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.988 (+/-0.017) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.982 (+/-0.026) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.988 (+/-0.017) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.983 (+/-0.026) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.988 (+/-0.017) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.983 (+/-0.026) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.974 (+/-0.012) for {'C': 1, 'kernel': 'linear'}\n",
            "0.974 (+/-0.012) for {'C': 10, 'kernel': 'linear'}\n",
            "0.974 (+/-0.012) for {'C': 100, 'kernel': 'linear'}\n",
            "0.974 (+/-0.012) for {'C': 1000, 'kernel': 'linear'}\n",
            "\n",
            "Detailed classification report on test-set:\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        89\n",
            "           1       0.97      1.00      0.98        90\n",
            "           2       0.99      0.98      0.98        92\n",
            "           3       1.00      0.99      0.99        93\n",
            "           4       1.00      1.00      1.00        76\n",
            "           5       0.99      0.98      0.99       108\n",
            "           6       0.99      1.00      0.99        89\n",
            "           7       0.99      1.00      0.99        78\n",
            "           8       1.00      0.98      0.99        92\n",
            "           9       0.99      0.99      0.99        92\n",
            "\n",
            "    accuracy                           0.99       899\n",
            "   macro avg       0.99      0.99      0.99       899\n",
            "weighted avg       0.99      0.99      0.99       899\n",
            "\n",
            "\n",
            "# Tuning hyper-parameters for recall\n",
            "\n",
            "Best parameters set found on train set:\n",
            "\n",
            "{'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "\n",
            "Grid scores on train set:\n",
            "\n",
            "0.986 (+/-0.019) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.957 (+/-0.028) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.987 (+/-0.019) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.981 (+/-0.028) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.987 (+/-0.019) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.982 (+/-0.026) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.987 (+/-0.019) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.982 (+/-0.026) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.971 (+/-0.010) for {'C': 1, 'kernel': 'linear'}\n",
            "0.971 (+/-0.010) for {'C': 10, 'kernel': 'linear'}\n",
            "0.971 (+/-0.010) for {'C': 100, 'kernel': 'linear'}\n",
            "0.971 (+/-0.010) for {'C': 1000, 'kernel': 'linear'}\n",
            "\n",
            "Detailed classification report on test-set:\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        89\n",
            "           1       0.97      1.00      0.98        90\n",
            "           2       0.99      0.98      0.98        92\n",
            "           3       1.00      0.99      0.99        93\n",
            "           4       1.00      1.00      1.00        76\n",
            "           5       0.99      0.98      0.99       108\n",
            "           6       0.99      1.00      0.99        89\n",
            "           7       0.99      1.00      0.99        78\n",
            "           8       1.00      0.98      0.99        92\n",
            "           9       0.99      0.99      0.99        92\n",
            "\n",
            "    accuracy                           0.99       899\n",
            "   macro avg       0.99      0.99      0.99       899\n",
            "weighted avg       0.99      0.99      0.99       899\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptIc2k2WmgOK",
        "colab_type": "text"
      },
      "source": [
        "## Random-Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKTpBmlkci0o",
        "colab_type": "code",
        "outputId": "ee92a6a1-81d8-420c-86eb-bbabb47a4551",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        }
      },
      "source": [
        "# RandomSearch CV\n",
        "# Source: https://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from time import time\n",
        "import scipy.stats as stats\n",
        "from sklearn.utils.fixes import loguniform\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# get some data\n",
        "X, y = load_digits(return_X_y=True)\n",
        "\n",
        "# build a classifier\n",
        "clf = SGDClassifier(loss='hinge', penalty='elasticnet',\n",
        "                    fit_intercept=True)\n",
        "\n",
        "\n",
        "# Utility function to report best scores\n",
        "def report(results, n_top=3):\n",
        "    for i in range(1, n_top + 1):\n",
        "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
        "        for candidate in candidates:\n",
        "            print(\"Model with rank: {0}\".format(i))\n",
        "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\"\n",
        "                  .format(results['mean_test_score'][candidate],\n",
        "                          results['std_test_score'][candidate]))\n",
        "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
        "            print(\"\")\n",
        "\n",
        "\n",
        "# specify parameters and distributions to sample from\n",
        "param_dist = {'average': [True, False],\n",
        "              'l1_ratio': stats.uniform(0, 1),\n",
        "              'alpha': loguniform(1e-4, 1e0)}\n",
        "\n",
        "# run randomized search\n",
        "n_iter_search = 20\n",
        "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
        "                                   n_iter=n_iter_search)\n",
        "\n",
        "start = time()\n",
        "random_search.fit(X, y)\n",
        "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
        "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
        "report(random_search.cv_results_)\n",
        "\n",
        "# use a full grid over all parameters\n",
        "param_grid = {'average': [True, False],\n",
        "              'l1_ratio': np.linspace(0, 1, num=10),\n",
        "              'alpha': np.power(10, np.arange(-4, 1, dtype=float))}\n",
        "\n",
        "# run grid search\n",
        "grid_search = GridSearchCV(clf, param_grid=param_grid)\n",
        "start = time()\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
        "      % (time() - start, len(grid_search.cv_results_['params'])))\n",
        "report(grid_search.cv_results_)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "RandomizedSearchCV took 40.75 seconds for 20 candidates parameter settings.\n",
            "Model with rank: 1\n",
            "Mean validation score: 0.928 (std: 0.027)\n",
            "Parameters: {'alpha': 0.00023985835751809425, 'average': True, 'l1_ratio': 0.27293545885292814}\n",
            "\n",
            "Model with rank: 2\n",
            "Mean validation score: 0.925 (std: 0.025)\n",
            "Parameters: {'alpha': 0.0005649400414302602, 'average': True, 'l1_ratio': 0.38395942078191736}\n",
            "\n",
            "Model with rank: 3\n",
            "Mean validation score: 0.921 (std: 0.024)\n",
            "Parameters: {'alpha': 0.10670145618014917, 'average': False, 'l1_ratio': 0.1619293637603787}\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "GridSearchCV took 129.86 seconds for 100 candidate parameter settings.\n",
            "Model with rank: 1\n",
            "Mean validation score: 0.932 (std: 0.031)\n",
            "Parameters: {'alpha': 1.0, 'average': True, 'l1_ratio': 0.0}\n",
            "\n",
            "Model with rank: 2\n",
            "Mean validation score: 0.929 (std: 0.031)\n",
            "Parameters: {'alpha': 0.0001, 'average': True, 'l1_ratio': 0.6666666666666666}\n",
            "\n",
            "Model with rank: 3\n",
            "Mean validation score: 0.927 (std: 0.030)\n",
            "Parameters: {'alpha': 0.0001, 'average': True, 'l1_ratio': 0.1111111111111111}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i876dSrZml3Z",
        "colab_type": "text"
      },
      "source": [
        "## Bayesian Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkGvqHemcldf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#HyperOPT: Bayesian Optimization\n",
        "\n",
        "#Refer: https://hyperopt.github.io/hyperopt-sklearn/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk1USHp-8xgG",
        "colab_type": "code",
        "outputId": "62b132a9-1235-4656-cb15-75e971995d19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "!pip install hpsklearn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hpsklearn\n",
            "  Downloading https://files.pythonhosted.org/packages/ce/cb/61b99f73621e2692abd0e730f7888a9983d01f626868336fa1db1d57bc1e/hpsklearn-0.1.0.tar.gz\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (from hpsklearn) (0.1.2)\n",
            "Collecting nose\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/d8/dd071918c040f50fa1cf80da16423af51ff8ce4a0f2399b7bf8de45ac3d9/nose-1.3.7-py3-none-any.whl (154kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 3.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hpsklearn) (1.18.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from hpsklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hpsklearn) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt->hpsklearn) (2.4)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt->hpsklearn) (3.10.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt->hpsklearn) (0.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt->hpsklearn) (4.38.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt->hpsklearn) (1.12.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->hpsklearn) (0.14.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt->hpsklearn) (4.4.2)\n",
            "Building wheels for collected packages: hpsklearn\n",
            "  Building wheel for hpsklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hpsklearn: filename=hpsklearn-0.1.0-cp36-none-any.whl size=23914 sha256=13bf2e15a918e9ef63c180103d397184158bd36839f5f6d1476f3b7bb3835853\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/ee/c4/3c267cbf78f0905434ee36b915d97a20610ad3af7ff3c75852\n",
            "Successfully built hpsklearn\n",
            "Installing collected packages: nose, hpsklearn\n",
            "Successfully installed hpsklearn-0.1.0 nose-1.3.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltVbSkgW9ulD",
        "colab_type": "code",
        "outputId": "627aaf7c-c64c-4ef7-f0b9-2d3286530bfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from hpsklearn import HyperoptEstimator"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARN: OMP_NUM_THREADS=None =>\n",
            "... If you are using openblas if you are using openblas set OMP_NUM_THREADS=1 or risk subprocess calls hanging indefinitely\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIuCxbYtJhZW",
        "colab_type": "code",
        "outputId": "1f57fd1d-a775-4cf2-fdab-7294aa4cb377",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "source": [
        "from hpsklearn import HyperoptEstimator, any_classifier\n",
        "from sklearn.datasets import fetch_openml\n",
        "from hyperopt import tpe\n",
        "import numpy as np\n",
        "\n",
        "# Download the data and split into training and test sets\n",
        "\n",
        "digits = fetch_openml('mnist_784')\n",
        "\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "test_size = int( 0.2 * len( y ) )\n",
        "np.random.seed( 100 )\n",
        "indices = np.random.permutation(len(X))\n",
        "X_train = X[ indices[:-test_size]]\n",
        "y_train = y[ indices[:-test_size]]\n",
        "X_test = X[ indices[-test_size:]]\n",
        "y_test = y[ indices[-test_size:]]\n",
        "\n",
        "estim = HyperoptEstimator( classifier=any_classifier('clf'),  \n",
        "                            algo=tpe.suggest, trial_timeout=300)\n",
        "\n",
        "estim.fit( X_train, y_train )\n",
        "\n",
        "print( estim.score( X_test, y_test ) )\n",
        "# <<show score here>>\n",
        "print( estim.best_model() )\n",
        "# <<show model here>>"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:05<00:00,  5.30s/it, best loss: 0.2952678571428572]\n",
            "100%|██████████| 1/1 [05:00<00:00, 300.19s/it, best loss: 0.2952678571428572]\n",
            "100%|██████████| 1/1 [00:28<00:00, 28.41s/it, best loss: 0.04062500000000002]\n",
            "100%|██████████| 1/1 [00:37<00:00, 37.41s/it, best loss: 0.04062500000000002]\n",
            "100%|██████████| 1/1 [00:41<00:00, 41.83s/it, best loss: 0.04062500000000002]\n",
            "100%|██████████| 1/1 [05:00<00:00, 300.18s/it, best loss: 0.04062500000000002]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s, best loss: ?]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
            "  FutureWarning)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [05:00<00:00, 300.15s/it, best loss: 0.04062500000000002]\n",
            "100%|██████████| 1/1 [02:26<00:00, 146.46s/it, best loss: 0.04062500000000002]\n",
            "100%|██████████| 1/1 [05:00<00:00, 300.19s/it, best loss: 0.04062500000000002]\n",
            "100%|██████████| 1/1 [03:42<00:00, 222.36s/it, best loss: 0.04062500000000002]\n",
            "0.9652857142857143\n",
            "{'learner': ExtraTreesClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
            "                     criterion='entropy', max_depth=None,\n",
            "                     max_features=0.5315656358125643, max_leaf_nodes=None,\n",
            "                     max_samples=None, min_impurity_decrease=0.0,\n",
            "                     min_impurity_split=None, min_samples_leaf=1,\n",
            "                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "                     n_estimators=23, n_jobs=1, oob_score=False, random_state=4,\n",
            "                     verbose=False, warm_start=False), 'preprocs': (Normalizer(copy=True, norm='l2'),), 'ex_preprocs': ()}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccWMFbYX94tk",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}